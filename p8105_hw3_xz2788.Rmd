---
title: "p8105_hw3_xz2788"
author: "Xiaoyue Zhang"
date: "10/4/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

Load dataset and package

```{r load_dataset_package}
library(p8105.datasets)
library(tidyverse)
library(patchwork)
data("brfss_smart2010")
```

Look at the data first

```{r overview_data, eval=FALSE}
str(brfss_smart2010)
head(brfss_smart2010)
tail(brfss_smart2010)
View(brfss_smart2010)
```

Tidy the data

```{r tidy_data}
brfss_smart2010_tidy = janitor::clean_names(brfss_smart2010) %>%
  filter(topic == "Overall Health") %>% 
  rename(state = locationabbr, county = locationdesc) %>% 
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"))
```

Convert the "response" variable into a factor

```{r convert_to_factor}
brfss_smart2010_tidy$response = factor(brfss_smart2010_tidy$response, 
                      levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
```

## which states were observed at 7 locations in 2002

```{r state_at_seven_locations}
brfss_smart2010_tidy %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(geo_location)) %>% 
  filter(n_location == 7)
```

There are three states were observed at 7 locations. These are:
"Connecticut (CT)", "Florida (FL)", "North Carolina (NC)"

## "Spaghetti plot"

```{r spaghetti_plot}
brfss_smart2010_tidy %>% 
  group_by(state, year) %>% 
  summarize(n_locat = sum(n_distinct(geo_location, na.rm = TRUE))) %>% 
  ggplot(aes(x = year, y = n_locat)) + 
  geom_line(aes(color = state)) +
  theme_bw() +
  labs(
    title = "Number of locations in each state 2002-2010",
    x = "Year",
    y = "Number of locations",
    caption = "Data from the brfss dataset"
  ) +
  theme(legend.position = "bottom") +
  guides(col = guide_legend(nrow = 3))
```

FL state has the highest number of locations in the year of 2007, but some of these stations did not conduct the study every year. Most of the state only have less than 10 locations.

## Making a table

```{r make_table}
filter(brfss_smart2010_tidy, 
       state == "NY",
       year %in% c(2002, 2006, 2010),
       response == "Excellent") %>%
  group_by(year) %>% 
  summarize(sd_prop = sd(data_value),
            mean_prop = mean(data_value)) %>% 
  knitr::kable(digits = 2)
```

From 2002 to 2010, the mean proportion of "Excellence" response decreased from 24% to 22.7% in New York City, and the standard deviations were also decreasing which showed that different proportions of "Excellence" in different locations in NY was smaller.

## Five-panel plot

```{r make_five_panel_plot}
brfss_smart2010_tidy %>% 
  group_by(state, year, response) %>% 
  summarize(average_prop = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = average_prop)) +
  geom_line(aes(color = state)) +
  facet_grid(. ~ response) +
  theme_bw() +
  labs(
    title = "Proportion of each response 2002-2010",
    x = "Year",
    y = "Average proportion of response (%)",
    caption = "Data from the brfss dataset"
  ) +
   theme(legend.position = "bottom") +
  guides(col = guide_legend(nrow = 3))
```

Through the years, the proportion of each response floated up and down, but most of people have "Very good", "Good" and "Excellent" response. Those states which had high proportion of "Fair" or "Poor" were also likely to have low proportion of good positive responses.

# Problem 2

Load the data

```{r}
data("instacart")
```

View the data first

```{r p2_data, eval=FALSE}
View(instacart)
head(instacart)
tail(instacart)
str(instacart)
dim(instacart)
skimr::skim(instacart)
```

There are 1,384,617 observations and 15 variables in this dataset. Each row represents a product from an online order and contains information about the time of the order, product of the order, the previous ordering history from this same customer. Only the evaluation set that the order belongs to, product name, aisle identifier and name of department are character variables. Others are integers. There are no missing values in the dataset.

For example, in the first row, it shows the product "Bulgarian Yogurt" ordered by the user with ID "112108" and it was ordered from the cart at 10:00 am on Thursday. This customer also bought this same product 9 days ago.

## How many aisles and which are the most items ordered from?

```{r distinct_aisles}
n_distinct(instacart$aisle_id, na.rm = TRUE)
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) %>% 
  filter(n_items == max(n_items))
```

There are in total 134 aisles and the aisle of "fresh vegetables" is the most items ordered from.

## Plot of the number of items ordered in each aisle

Because of the large sample, divide into 3 groups for plotting

```{r divide_group}
desc_instacart = instacart %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) %>% 
  arrange(-n_items) 
group_1 = desc_instacart[1:45,]
group_2 = desc_instacart[45:90,]
group_3 = desc_instacart[91:134,]
```

Plotting three plot seperately and then combine

```{r plot_ailse}
group_1 %>%  
  transform(aisle = reorder(aisle, -n_items)) %>% 
  ggplot(aes(x = aisle, y = n_items)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  geom_text(aes(label = n_items)) +
  labs(
    title = "Number of products ordered in each aisle",
    x = "Aisle",
    y = "Number of products",
    caption = "Data from the instacart dataset"
  ) +
  coord_flip()

group_2 %>%  
  transform(aisle = reorder(aisle, -n_items)) %>% 
  ggplot(aes(x = aisle, y = n_items)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  geom_text(aes(label = n_items)) +
  labs(
    title = "Number of products ordered in each aisle",
    x = "Aisle",
    y = "Number of products",
    caption = "Data from the instacart dataset"
  ) +
  coord_flip()

group_3 %>%  
  transform(aisle = reorder(aisle, -n_items)) %>% 
  ggplot(aes(x = aisle, y = n_items)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  geom_text(aes(label = n_items)) +
  labs(
    title = "Number of products ordered in each aisle",
    x = "Aisle",
    y = "Number of products",
    caption = "Data from the instacart dataset"
  ) +
  coord_flip()
```

Comment:
The most popular aisles are "fresh fruits" and "fresh vegetables", there are over 150000 items ordered from each aisle. The least popular aisle is "beauty" which is less than 300.

## Table showing the most popular item in three specific aisles

```{r table_of_pop_item}
instacart %>% 
  filter(aisle %in%
           c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_products = n()) %>% 
  filter(n_products == max(n_products)) %>% 
  knitr::kable()
```

The most popular item in "baking ingredients" is "Light Brown Sugar" with 499 ordering histories.
The most popular item in "dog food care" is "Snack Sticks Chicken & Rice Recipe Dog Treats" with 30 ordering histories.
The most popular item in "packaged vegetables fruits" is "Organic Baby Spinach" with 9784 ordering histories.

## Table showing the mean hour of the day

```{r table_of_mean_hour}
instacart$order_dow = recode(instacart$order_dow, `0` = "Sunday", `6` = "Saturday", `5` = "Friday", `4` = "Thursday", `3` = "Wednesday", `2` = "Tuesday", `1` = "Monday") 
instacart$order_dow = factor(instacart$order_dow, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = round(mean(order_hour_of_day))) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable() 
```

People usually buy coffee ice cream in the afternoon of the day, just after lunch, but for Friday, the time is a little bit earlier. The ordering time of pink lady apples are earlier than that of coffee ice cream, almost at noon. The ordering time among days of the week does not vary a lot.

# Problem 3

Load the data

```{r}
data("ny_noaa")
```

View the data first

```{r p3_data, eval=FALSE}
head(ny_noaa)
tail(ny_noaa)
str(ny_noaa)
skimr::skim(ny_noaa)
View(ny_noaa)
```

There are 2,595,176 observations and 7 variables. It mainly contains the information of the date, maximum and minimum temperature (tenths of degrees C), total precipitation (tenths of mm) of the day, snow fall (mm) and snow depth (mm). "date" is a Date variable, station ID, maximum temperature, minimum temperature are character variables and all of the others are integers. Except ID and date, all the variables have missing values. Because it didn't rain or snow for some days in some stations and also some stations only collect part of the variables. Missing value can be an issue when one station does not collect the data of some variables, R will consider it as zero which means no rain or snow. In this kind of situation, NA can effect the result.

Tidy the data 

```{r tidy_data_p3}
ny_noaa_tidy = ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmax = as.numeric(tmax), tmin = as.numeric(tmin)) %>% 
  mutate(prcp = prcp/10, tmax = tmax/10, tmin = tmin/10)
```

## Which value of snowfall is most observed?

```{r}
ny_noaa_tidy %>% 
  filter(!is.na(snow)) %>%
  group_by(snow) %>% 
  summarize(n_value = n()) %>% 
  filter(n_value == max(n_value))
```

Thus, for snowfall, the most commonly observed value is 0. Because I firstly filter the all the data from "snow" which is not an NA. The left data of snowfall is actually the data that stations documented instead of missing value. 0 means it didn't snow at that day.

## Making a plot of average temperatures in January and July

```{r plot_of_average_temp}
ny_noaa_tidy %>% 
  filter(month %in% c("01", "07")) %>% 
  ggplot(aes(y = year, x = (tmax + tmin)/2)) +
  geom_point(aes(color = id)) +
  facet_grid(~month) +
  theme_bw() +
  theme(legend.position = "none") 
```

